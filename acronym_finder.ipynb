{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import collections\n",
    "#from gensim.parsing.preprocessing import remove_stopwords\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import itertools \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag.stanford import StanfordPOSTagger\n",
    "from string import punctuation\n",
    "from textblob import TextBlob\n",
    "\n",
    "from nltk import ngrams\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#java path for standford tagger\n",
    "java_path = \"C:\\Program Files (x86)\\Java\\jre1.8.0_201\"\n",
    "os.environ['JAVAHOME'] = java_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Standford Spanish POS tagger\n",
    "_model_filename = r'C:/Users/cx02274/Documents/master/stanford-postagger/models/spanish-ud.tagger'\n",
    "_path_to_jar = r'C:/Users/cx02274/Documents/master/stanford-postagger/stanford-postagger-4.1.0.jar'\n",
    "st = StanfordPOSTagger(model_filename=_model_filename, path_to_jar=_path_to_jar,encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_texts(path):\n",
    "    data = []\n",
    "    file_name = os.listdir(path)\n",
    "\n",
    "    for name in file_name:\n",
    "        if name.endswith('.txt'):\n",
    "            with open(path + name,encoding=\"utf8\") as f:\n",
    "                text = f.read()\n",
    "                data.append({'nombre':name.replace('.txt',''), 'texto':text})\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainning\n",
    "\n",
    "318 clinical cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_abbr = pd.read_csv(\"../datasets/trainning_set/clinical_cases.abbreviations.training_set.tsv\", sep = '\\t')\n",
    "train_met = pd.read_csv(\"../datasets/trainning_set/clinical_cases.metadata.training_set.tsv\", sep = '\\t')\n",
    "train_rel = pd.read_csv(\"../datasets/trainning_set/clinical_cases.relations.training_set.tsv\", sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_met = train_met.rename(columns = {'# Document_ID': 'doc_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>Case_ID</th>\n",
       "      <th>ISSN</th>\n",
       "      <th>Date</th>\n",
       "      <th>Source</th>\n",
       "      <th>Full_Text_Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S1139-76322015000500009-1.txt</td>\n",
       "      <td>1.txt</td>\n",
       "      <td>1139-7632</td>\n",
       "      <td>2015-12-01</td>\n",
       "      <td>Pediatría Atención Primaria  v.17 n.68 2015</td>\n",
       "      <td>http://scielo.isciii.es/scielo.php?script=sci_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S1130-05582008000400007-2.txt</td>\n",
       "      <td>2.txt</td>\n",
       "      <td>1130-0558</td>\n",
       "      <td>2008-08-01</td>\n",
       "      <td>Revista Española de Cirugía Oral y Maxilofacia...</td>\n",
       "      <td>http://scielo.isciii.es/scielo.php?script=sci_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S0210-48062006000100012-1.txt</td>\n",
       "      <td>1.txt</td>\n",
       "      <td>0210-4806</td>\n",
       "      <td>2006-01-01</td>\n",
       "      <td>Actas Urológicas Españolas  v.30 n.1 2006</td>\n",
       "      <td>http://scielo.isciii.es/scielo.php?script=sci_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>S0213-12852003000500002-1.txt</td>\n",
       "      <td>1.txt</td>\n",
       "      <td>0213-1285</td>\n",
       "      <td>2003-10-01</td>\n",
       "      <td>Avances en Odontoestomatología  v.19 n.5 2003</td>\n",
       "      <td>http://scielo.isciii.es/scielo.php?script=sci_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>S0212-71992005000400007-1.txt</td>\n",
       "      <td>1.txt</td>\n",
       "      <td>0212-7199</td>\n",
       "      <td>2005-04-01</td>\n",
       "      <td>Anales de Medicina Interna  v.22 n.4 2005</td>\n",
       "      <td>http://scielo.isciii.es/scielo.php?script=sci_...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          doc_id Case_ID       ISSN        Date  \\\n",
       "0  S1139-76322015000500009-1.txt   1.txt  1139-7632  2015-12-01   \n",
       "1  S1130-05582008000400007-2.txt   2.txt  1130-0558  2008-08-01   \n",
       "2  S0210-48062006000100012-1.txt   1.txt  0210-4806  2006-01-01   \n",
       "3  S0213-12852003000500002-1.txt   1.txt  0213-1285  2003-10-01   \n",
       "4  S0212-71992005000400007-1.txt   1.txt  0212-7199  2005-04-01   \n",
       "\n",
       "                                              Source  \\\n",
       "0        Pediatría Atención Primaria  v.17 n.68 2015   \n",
       "1  Revista Española de Cirugía Oral y Maxilofacia...   \n",
       "2          Actas Urológicas Españolas  v.30 n.1 2006   \n",
       "3      Avances en Odontoestomatología  v.19 n.5 2003   \n",
       "4          Anales de Medicina Interna  v.22 n.4 2005   \n",
       "\n",
       "                                      Full_Text_Link  \n",
       "0  http://scielo.isciii.es/scielo.php?script=sci_...  \n",
       "1  http://scielo.isciii.es/scielo.php?script=sci_...  \n",
       "2  http://scielo.isciii.es/scielo.php?script=sci_...  \n",
       "3  http://scielo.isciii.es/scielo.php?script=sci_...  \n",
       "4  http://scielo.isciii.es/scielo.php?script=sci_...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_met.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rel = train_rel.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rel.columns = ['# Document_ID', 'Mention_A_type', 'Mention_A_StartOffset',\n",
    "      'Mention_A', 'Relation_type', 'Mention_B_type',\n",
    "       'Mention_B_StartOffset', 'Mention_B_EndOffset', 'Mention_B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rel = train_rel.rename(columns = {'# Document_ID': 'doc_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>Mention_A_type</th>\n",
       "      <th>Mention_A_StartOffset</th>\n",
       "      <th>Mention_A</th>\n",
       "      <th>Relation_type</th>\n",
       "      <th>Mention_B_type</th>\n",
       "      <th>Mention_B_StartOffset</th>\n",
       "      <th>Mention_B_EndOffset</th>\n",
       "      <th>Mention_B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S1130-01082009000400014-1</td>\n",
       "      <td>SHORT_FORM</td>\n",
       "      <td>476</td>\n",
       "      <td>NPT</td>\n",
       "      <td>SHORT-LONG</td>\n",
       "      <td>LONG_FORM</td>\n",
       "      <td>454.0</td>\n",
       "      <td>474.0</td>\n",
       "      <td>nutrición parenteral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S1130-63432016000100009-1</td>\n",
       "      <td>SHORT_FORM</td>\n",
       "      <td>614</td>\n",
       "      <td>NIHSS</td>\n",
       "      <td>SHORT-LONG</td>\n",
       "      <td>LONG_FORM</td>\n",
       "      <td>621.0</td>\n",
       "      <td>662.0</td>\n",
       "      <td>National Institute of Health Stroke Scale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S1139-76322017000200007-1</td>\n",
       "      <td>SHORT_FORM</td>\n",
       "      <td>1145</td>\n",
       "      <td>CMV</td>\n",
       "      <td>SHORT-LONG</td>\n",
       "      <td>LONG_FORM</td>\n",
       "      <td>1128.0</td>\n",
       "      <td>1143.0</td>\n",
       "      <td>citomegalovirus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>S1139-76322017000200007-1</td>\n",
       "      <td>SHORT_FORM</td>\n",
       "      <td>1243</td>\n",
       "      <td>VSG</td>\n",
       "      <td>SHORT-LONG</td>\n",
       "      <td>LONG_FORM</td>\n",
       "      <td>1206.0</td>\n",
       "      <td>1241.0</td>\n",
       "      <td>velocidad de sedimentación globular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>S1139-76322017000200007-1</td>\n",
       "      <td>SHORT_FORM</td>\n",
       "      <td>1300</td>\n",
       "      <td>IGRA</td>\n",
       "      <td>SHORT-LONG</td>\n",
       "      <td>LONG_FORM</td>\n",
       "      <td>1267.0</td>\n",
       "      <td>1298.0</td>\n",
       "      <td>interferon-gamma release assays</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      doc_id Mention_A_type Mention_A_StartOffset Mention_A  \\\n",
       "0  S1130-01082009000400014-1     SHORT_FORM                   476       NPT   \n",
       "1  S1130-63432016000100009-1     SHORT_FORM                   614     NIHSS   \n",
       "2  S1139-76322017000200007-1     SHORT_FORM                  1145       CMV   \n",
       "3  S1139-76322017000200007-1     SHORT_FORM                  1243       VSG   \n",
       "4  S1139-76322017000200007-1     SHORT_FORM                  1300      IGRA   \n",
       "\n",
       "  Relation_type Mention_B_type  Mention_B_StartOffset  Mention_B_EndOffset  \\\n",
       "0    SHORT-LONG      LONG_FORM                  454.0                474.0   \n",
       "1    SHORT-LONG      LONG_FORM                  621.0                662.0   \n",
       "2    SHORT-LONG      LONG_FORM                 1128.0               1143.0   \n",
       "3    SHORT-LONG      LONG_FORM                 1206.0               1241.0   \n",
       "4    SHORT-LONG      LONG_FORM                 1267.0               1298.0   \n",
       "\n",
       "                                   Mention_B  \n",
       "0                       nutrición parenteral  \n",
       "1  National Institute of Health Stroke Scale  \n",
       "2                            citomegalovirus  \n",
       "3        velocidad de sedimentación globular  \n",
       "4            interferon-gamma release assays  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_rel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_abbr = train_abbr.rename(columns = {'# Document_ID': 'doc_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw = read_texts(\"../datasets/trainning_set/training_set.raw_text/txt/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw = train_raw.rename(columns = {'nombre': 'doc_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>texto</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S0004-06142005000900013-1</td>\n",
       "      <td>Se trata de una mujer de 29 años sometida a un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S0004-06142005000900015-1</td>\n",
       "      <td>Varón de 36 años, sin antecedentes de interés,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S0004-06142005000900016-1</td>\n",
       "      <td>Mujer de 29 años con antecedentes de ulcus duo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>S0004-06142005001000011-1</td>\n",
       "      <td>Varón de 58 años de edad en el momento del tra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>S0004-06142005001000011-3</td>\n",
       "      <td>Mujer de 42 años en el momento de someterse a ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      doc_id  \\\n",
       "0  S0004-06142005000900013-1   \n",
       "1  S0004-06142005000900015-1   \n",
       "2  S0004-06142005000900016-1   \n",
       "3  S0004-06142005001000011-1   \n",
       "4  S0004-06142005001000011-3   \n",
       "\n",
       "                                               texto  \n",
       "0  Se trata de una mujer de 29 años sometida a un...  \n",
       "1  Varón de 36 años, sin antecedentes de interés,...  \n",
       "2  Mujer de 29 años con antecedentes de ulcus duo...  \n",
       "3  Varón de 58 años de edad en el momento del tra...  \n",
       "4  Mujer de 42 años en el momento de someterse a ...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sub-track 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Found abbreviations (Short Forms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize text\n",
    "train_raw['tokens'] = train_raw['texto'].map(lambda x: word_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove stop words\n",
    "#nltk.download('stopwords')\n",
    "swords = list(set(stopwords.words('spanish')))\n",
    "train_raw['texto_clean'] = train_raw['tokens'].map(lambda x: ' '.join([w for w in x if w not in swords]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove white spaces\n",
    "train_raw['texto_clean'] = train_raw['texto_clean'].map(lambda x: x.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split text in sentences\n",
    "train_raw['texto_sentences'] = train_raw['texto_clean'].map(lambda x: re.split(r\"\\.|\\?|\\!\", x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "#POS tagging\n",
    "train_raw['texto_POS'] = train_raw['texto_clean'].map(lambda x: st.tag(nltk.word_tokenize(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>texto</th>\n",
       "      <th>tokens</th>\n",
       "      <th>texto_clean</th>\n",
       "      <th>texto_sentences</th>\n",
       "      <th>texto_POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S0004-06142005000900013-1</td>\n",
       "      <td>Se trata de una mujer de 29 años sometida a un...</td>\n",
       "      <td>[Se, trata, de, una, mujer, de, 29, años, some...</td>\n",
       "      <td>Se trata mujer 29 años sometida estudio ecográ...</td>\n",
       "      <td>[Se trata mujer 29 años sometida estudio ecogr...</td>\n",
       "      <td>[(Se, PRON), (trata, VERB), (mujer, NOUN), (29...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S0004-06142005000900015-1</td>\n",
       "      <td>Varón de 36 años, sin antecedentes de interés,...</td>\n",
       "      <td>[Varón, de, 36, años, ,, sin, antecedentes, de...</td>\n",
       "      <td>Varón 36 años , antecedentes interés , estudia...</td>\n",
       "      <td>[Varón 36 años , antecedentes interés , estudi...</td>\n",
       "      <td>[(Varón, PROPN), (36, NUM), (años, NOUN), (,, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S0004-06142005000900016-1</td>\n",
       "      <td>Mujer de 29 años con antecedentes de ulcus duo...</td>\n",
       "      <td>[Mujer, de, 29, años, con, antecedentes, de, u...</td>\n",
       "      <td>Mujer 29 años antecedentes ulcus duodenal estr...</td>\n",
       "      <td>[Mujer 29 años antecedentes ulcus duodenal est...</td>\n",
       "      <td>[(Mujer, NOUN), (29, NUM), (años, NOUN), (ante...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>S0004-06142005001000011-1</td>\n",
       "      <td>Varón de 58 años de edad en el momento del tra...</td>\n",
       "      <td>[Varón, de, 58, años, de, edad, en, el, moment...</td>\n",
       "      <td>Varón 58 años edad momento trasplante , 5 octu...</td>\n",
       "      <td>[Varón 58 años edad momento trasplante , 5 oct...</td>\n",
       "      <td>[(Varón, PROPN), (58, NUM), (años, NOUN), (eda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>S0004-06142005001000011-3</td>\n",
       "      <td>Mujer de 42 años en el momento de someterse a ...</td>\n",
       "      <td>[Mujer, de, 42, años, en, el, momento, de, som...</td>\n",
       "      <td>Mujer 42 años momento someterse trasplante hep...</td>\n",
       "      <td>[Mujer 42 años momento someterse trasplante he...</td>\n",
       "      <td>[(Mujer, NOUN), (42, NUM), (años, NOUN), (mome...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      doc_id  \\\n",
       "0  S0004-06142005000900013-1   \n",
       "1  S0004-06142005000900015-1   \n",
       "2  S0004-06142005000900016-1   \n",
       "3  S0004-06142005001000011-1   \n",
       "4  S0004-06142005001000011-3   \n",
       "\n",
       "                                               texto  \\\n",
       "0  Se trata de una mujer de 29 años sometida a un...   \n",
       "1  Varón de 36 años, sin antecedentes de interés,...   \n",
       "2  Mujer de 29 años con antecedentes de ulcus duo...   \n",
       "3  Varón de 58 años de edad en el momento del tra...   \n",
       "4  Mujer de 42 años en el momento de someterse a ...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [Se, trata, de, una, mujer, de, 29, años, some...   \n",
       "1  [Varón, de, 36, años, ,, sin, antecedentes, de...   \n",
       "2  [Mujer, de, 29, años, con, antecedentes, de, u...   \n",
       "3  [Varón, de, 58, años, de, edad, en, el, moment...   \n",
       "4  [Mujer, de, 42, años, en, el, momento, de, som...   \n",
       "\n",
       "                                         texto_clean  \\\n",
       "0  Se trata mujer 29 años sometida estudio ecográ...   \n",
       "1  Varón 36 años , antecedentes interés , estudia...   \n",
       "2  Mujer 29 años antecedentes ulcus duodenal estr...   \n",
       "3  Varón 58 años edad momento trasplante , 5 octu...   \n",
       "4  Mujer 42 años momento someterse trasplante hep...   \n",
       "\n",
       "                                     texto_sentences  \\\n",
       "0  [Se trata mujer 29 años sometida estudio ecogr...   \n",
       "1  [Varón 36 años , antecedentes interés , estudi...   \n",
       "2  [Mujer 29 años antecedentes ulcus duodenal est...   \n",
       "3  [Varón 58 años edad momento trasplante , 5 oct...   \n",
       "4  [Mujer 42 años momento someterse trasplante he...   \n",
       "\n",
       "                                           texto_POS  \n",
       "0  [(Se, PRON), (trata, VERB), (mujer, NOUN), (29...  \n",
       "1  [(Varón, PROPN), (36, NUM), (años, NOUN), (,, ...  \n",
       "2  [(Mujer, NOUN), (29, NUM), (años, NOUN), (ante...  \n",
       "3  [(Varón, PROPN), (58, NUM), (años, NOUN), (eda...  \n",
       "4  [(Mujer, NOUN), (42, NUM), (años, NOUN), (mome...  "
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect acronym with patterns and heuristic search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = [lambda concept: sum(x.isdigit() for x in concept) <= 1, # could have one digit\n",
    "        lambda concept: any(x.isalpha() for x in concept),  # must have at least one letter\n",
    "        lambda concept: sum(x.isupper() for x in concept) >= 2,  # must have at least one uppercase\n",
    "        lambda concept: sum(x.isspace() for x in concept) <= 1,  # could have just one white space\n",
    "        lambda concept: (len(concept) >= 2 and len(concept) <= 10), # must be more than 2 characters and at least 10 characters\n",
    "#         lambda concept: concept.startswith('(')\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw['acronym'] = train_raw['texto_POS'].map(lambda x: [a[0] for a in x if (all(rule(a[0]) for rule in rules) and a[1] == 'PROPN')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>texto</th>\n",
       "      <th>tokens</th>\n",
       "      <th>texto_clean</th>\n",
       "      <th>texto_sentences</th>\n",
       "      <th>texto_POS</th>\n",
       "      <th>acronym</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S0004-06142005000900013-1</td>\n",
       "      <td>Se trata de una mujer de 29 años sometida a un...</td>\n",
       "      <td>[Se, trata, de, una, mujer, de, 29, años, some...</td>\n",
       "      <td>Se trata mujer 29 años sometida estudio ecográ...</td>\n",
       "      <td>[Se trata mujer 29 años sometida estudio ecogr...</td>\n",
       "      <td>[(Se, PRON), (trata, VERB), (mujer, NOUN), (29...</td>\n",
       "      <td>[DAKO]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S0004-06142005000900015-1</td>\n",
       "      <td>Varón de 36 años, sin antecedentes de interés,...</td>\n",
       "      <td>[Varón, de, 36, años, ,, sin, antecedentes, de...</td>\n",
       "      <td>Varón 36 años , antecedentes interés , estudia...</td>\n",
       "      <td>[Varón 36 años , antecedentes interés , estudi...</td>\n",
       "      <td>[(Varón, PROPN), (36, NUM), (años, NOUN), (,, ...</td>\n",
       "      <td>[CT, MESNA]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S0004-06142005000900016-1</td>\n",
       "      <td>Mujer de 29 años con antecedentes de ulcus duo...</td>\n",
       "      <td>[Mujer, de, 29, años, con, antecedentes, de, u...</td>\n",
       "      <td>Mujer 29 años antecedentes ulcus duodenal estr...</td>\n",
       "      <td>[Mujer 29 años antecedentes ulcus duodenal est...</td>\n",
       "      <td>[(Mujer, NOUN), (29, NUM), (años, NOUN), (ante...</td>\n",
       "      <td>[UIV]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>S0004-06142005001000011-1</td>\n",
       "      <td>Varón de 58 años de edad en el momento del tra...</td>\n",
       "      <td>[Varón, de, 58, años, de, edad, en, el, moment...</td>\n",
       "      <td>Varón 58 años edad momento trasplante , 5 octu...</td>\n",
       "      <td>[Varón 58 años edad momento trasplante , 5 oct...</td>\n",
       "      <td>[(Varón, PROPN), (58, NUM), (años, NOUN), (eda...</td>\n",
       "      <td>[IRC, IgA, II, EEII, EEII, ROT, RCP, RMN, LCR,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>S0004-06142005001000011-3</td>\n",
       "      <td>Mujer de 42 años en el momento de someterse a ...</td>\n",
       "      <td>[Mujer, de, 42, años, en, el, momento, de, som...</td>\n",
       "      <td>Mujer 42 años momento someterse trasplante hep...</td>\n",
       "      <td>[Mujer 42 años momento someterse trasplante he...</td>\n",
       "      <td>[(Mujer, NOUN), (42, NUM), (años, NOUN), (mome...</td>\n",
       "      <td>[HDA, II, EEII, EID, EID, ROT, RCP, RMN, LCR, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      doc_id  \\\n",
       "0  S0004-06142005000900013-1   \n",
       "1  S0004-06142005000900015-1   \n",
       "2  S0004-06142005000900016-1   \n",
       "3  S0004-06142005001000011-1   \n",
       "4  S0004-06142005001000011-3   \n",
       "\n",
       "                                               texto  \\\n",
       "0  Se trata de una mujer de 29 años sometida a un...   \n",
       "1  Varón de 36 años, sin antecedentes de interés,...   \n",
       "2  Mujer de 29 años con antecedentes de ulcus duo...   \n",
       "3  Varón de 58 años de edad en el momento del tra...   \n",
       "4  Mujer de 42 años en el momento de someterse a ...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [Se, trata, de, una, mujer, de, 29, años, some...   \n",
       "1  [Varón, de, 36, años, ,, sin, antecedentes, de...   \n",
       "2  [Mujer, de, 29, años, con, antecedentes, de, u...   \n",
       "3  [Varón, de, 58, años, de, edad, en, el, moment...   \n",
       "4  [Mujer, de, 42, años, en, el, momento, de, som...   \n",
       "\n",
       "                                         texto_clean  \\\n",
       "0  Se trata mujer 29 años sometida estudio ecográ...   \n",
       "1  Varón 36 años , antecedentes interés , estudia...   \n",
       "2  Mujer 29 años antecedentes ulcus duodenal estr...   \n",
       "3  Varón 58 años edad momento trasplante , 5 octu...   \n",
       "4  Mujer 42 años momento someterse trasplante hep...   \n",
       "\n",
       "                                     texto_sentences  \\\n",
       "0  [Se trata mujer 29 años sometida estudio ecogr...   \n",
       "1  [Varón 36 años , antecedentes interés , estudi...   \n",
       "2  [Mujer 29 años antecedentes ulcus duodenal est...   \n",
       "3  [Varón 58 años edad momento trasplante , 5 oct...   \n",
       "4  [Mujer 42 años momento someterse trasplante he...   \n",
       "\n",
       "                                           texto_POS  \\\n",
       "0  [(Se, PRON), (trata, VERB), (mujer, NOUN), (29...   \n",
       "1  [(Varón, PROPN), (36, NUM), (años, NOUN), (,, ...   \n",
       "2  [(Mujer, NOUN), (29, NUM), (años, NOUN), (ante...   \n",
       "3  [(Varón, PROPN), (58, NUM), (años, NOUN), (eda...   \n",
       "4  [(Mujer, NOUN), (42, NUM), (años, NOUN), (mome...   \n",
       "\n",
       "                                             acronym  \n",
       "0                                             [DAKO]  \n",
       "1                                        [CT, MESNA]  \n",
       "2                                              [UIV]  \n",
       "3  [IRC, IgA, II, EEII, EEII, ROT, RCP, RMN, LCR,...  \n",
       "4  [HDA, II, EEII, EID, EID, ROT, RCP, RMN, LCR, ...  "
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('pCO2', 'PROPN')\n",
      "('HCO3', 'PROPN')\n",
      "('HLA', 'PROPN')\n"
     ]
    }
   ],
   "source": [
    "tokens = train_raw.iloc[40]['texto_POS']\n",
    "for token in tokens:\n",
    "    if all(rule(token[0]) for rule in rules) and token[1] == 'PROPN':\n",
    "        print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Paciente',\n",
       " 'de',\n",
       " '29',\n",
       " 'años',\n",
       " 'de',\n",
       " 'edad',\n",
       " 'que',\n",
       " 'acude',\n",
       " 'al',\n",
       " 'Servicio',\n",
       " 'de',\n",
       " 'Urgencias',\n",
       " 'de',\n",
       " 'nuestro',\n",
       " 'Hospital',\n",
       " 'ante',\n",
       " 'la',\n",
       " 'presencia',\n",
       " 'de',\n",
       " 'una',\n",
       " 'erección',\n",
       " 'levemente',\n",
       " 'dolorosa',\n",
       " 'de',\n",
       " 'diez',\n",
       " 'horas',\n",
       " 'de',\n",
       " 'evolución.',\n",
       " 'El',\n",
       " 'paciente',\n",
       " 'refiere',\n",
       " 'haber',\n",
       " 'presentado',\n",
       " 'cuatro',\n",
       " 'episodios',\n",
       " 'similares',\n",
       " 'en',\n",
       " 'los',\n",
       " 'últimos',\n",
       " 'tres',\n",
       " 'meses',\n",
       " 'pero',\n",
       " 'en',\n",
       " 'todos',\n",
       " 'ellos',\n",
       " 'la',\n",
       " 'erección',\n",
       " 'había',\n",
       " 'cedido',\n",
       " 'como',\n",
       " 'máximo',\n",
       " 'en',\n",
       " 'tres',\n",
       " 'o',\n",
       " 'cuatro',\n",
       " 'horas.',\n",
       " 'El',\n",
       " 'paciente',\n",
       " 'niega',\n",
       " 'el',\n",
       " 'antecedente',\n",
       " 'de',\n",
       " 'traumatismo',\n",
       " 'peneano,',\n",
       " 'el',\n",
       " 'uso',\n",
       " 'de',\n",
       " 'drogas',\n",
       " 'u',\n",
       " 'otros',\n",
       " 'fármacos',\n",
       " 'incluidas',\n",
       " 'inyecciones',\n",
       " 'intracavernosas.',\n",
       " 'Así',\n",
       " 'mismo',\n",
       " 'no',\n",
       " 'presenta',\n",
       " 'ningún',\n",
       " 'otro',\n",
       " 'síntoma',\n",
       " 'acompañante',\n",
       " 'como',\n",
       " 'astenia,',\n",
       " 'anorexia',\n",
       " 'o',\n",
       " 'fiebre.',\n",
       " 'Se',\n",
       " 'realiza',\n",
       " 'una',\n",
       " 'gasometría',\n",
       " 'de',\n",
       " 'la',\n",
       " 'sangre',\n",
       " 'extraída',\n",
       " 'de',\n",
       " 'los',\n",
       " 'cuerpos',\n",
       " 'cavernosos',\n",
       " 'obteniendo',\n",
       " 'resultados',\n",
       " 'analíticos',\n",
       " 'de',\n",
       " 'sangre',\n",
       " 'venosa',\n",
       " '(pCO2',\n",
       " '163,1',\n",
       " 'mmHg,',\n",
       " 'HCO3',\n",
       " '13,2',\n",
       " 'mEq/l',\n",
       " 'plasma).',\n",
       " 'Ante',\n",
       " 'este',\n",
       " 'priapismo',\n",
       " 'de',\n",
       " 'prolongada',\n",
       " 'duración',\n",
       " 'y',\n",
       " 'la',\n",
       " 'ausencia',\n",
       " 'de',\n",
       " 'indicios',\n",
       " 'que',\n",
       " 'nos',\n",
       " 'hagan',\n",
       " 'pensar',\n",
       " 'en',\n",
       " 'un',\n",
       " 'priapismo',\n",
       " 'de',\n",
       " 'alto',\n",
       " 'flujo,',\n",
       " 'procedemos',\n",
       " 'a',\n",
       " 'intentar',\n",
       " 'revertir',\n",
       " 'la',\n",
       " 'erección',\n",
       " 'con',\n",
       " 'la',\n",
       " 'mayor',\n",
       " 'brevedad',\n",
       " 'posible,',\n",
       " 'con',\n",
       " 'el',\n",
       " 'fin',\n",
       " 'de',\n",
       " 'calmar',\n",
       " 'el',\n",
       " 'dolor',\n",
       " 'y',\n",
       " 'evitar',\n",
       " 'un',\n",
       " 'daño',\n",
       " 'irreversible',\n",
       " 'en',\n",
       " 'los',\n",
       " 'cuerpos',\n",
       " 'cavernosos.',\n",
       " 'Se',\n",
       " 'punciona',\n",
       " 'con',\n",
       " 'un',\n",
       " 'catéter',\n",
       " 'intravenoso',\n",
       " 'de',\n",
       " '19',\n",
       " 'G',\n",
       " 'en',\n",
       " 'ambos',\n",
       " 'cuerpos',\n",
       " 'cavernosos,',\n",
       " 'aspirando',\n",
       " 'sangre',\n",
       " 'en',\n",
       " 'repetidas',\n",
       " 'ocasiones',\n",
       " 'y',\n",
       " 'se',\n",
       " 'deja',\n",
       " 'que',\n",
       " 'ésta',\n",
       " 'gotee.',\n",
       " 'Tras',\n",
       " 'este',\n",
       " 'primer',\n",
       " 'paso',\n",
       " 'no',\n",
       " 'se',\n",
       " 'obtiene',\n",
       " 'disminución',\n",
       " 'de',\n",
       " 'la',\n",
       " 'erección,',\n",
       " 'por',\n",
       " 'lo',\n",
       " 'que',\n",
       " 'se',\n",
       " 'prepara',\n",
       " 'una',\n",
       " 'solución',\n",
       " 'de',\n",
       " 'fenilefrina',\n",
       " '(agonista',\n",
       " 'alfaadrenérgico)',\n",
       " 'en',\n",
       " 'suero',\n",
       " 'salino',\n",
       " 'a',\n",
       " 'una',\n",
       " 'concentración',\n",
       " 'de',\n",
       " '0,1',\n",
       " 'mg/ml.',\n",
       " 'Se',\n",
       " 'inyecta',\n",
       " '2',\n",
       " 'ml',\n",
       " 'de',\n",
       " 'la',\n",
       " 'solución',\n",
       " 'en',\n",
       " 'un',\n",
       " 'cuerpo',\n",
       " 'cavernoso,',\n",
       " 'comprimiendo',\n",
       " 'posteriormente.',\n",
       " 'Se',\n",
       " 'repite',\n",
       " 'nuevamente',\n",
       " 'esta',\n",
       " 'maniobra',\n",
       " 'en',\n",
       " 'el',\n",
       " 'cuerpo',\n",
       " 'cavernoso',\n",
       " 'contralateral.',\n",
       " 'Ante',\n",
       " 'la',\n",
       " 'ausencia',\n",
       " 'de',\n",
       " 'respuesta,',\n",
       " 'se',\n",
       " 'realiza',\n",
       " 'una',\n",
       " 'nueva',\n",
       " 'solución',\n",
       " 'de',\n",
       " 'fenilefrina,',\n",
       " 'mezclando',\n",
       " '10',\n",
       " 'mg',\n",
       " 'de',\n",
       " 'fenilefrina',\n",
       " 'en',\n",
       " '11',\n",
       " 'de',\n",
       " 'suero',\n",
       " 'salino',\n",
       " 'y',\n",
       " 'se',\n",
       " 'van',\n",
       " 'introduciendo',\n",
       " '20-30',\n",
       " 'ml',\n",
       " 'de',\n",
       " 'esta',\n",
       " 'solución',\n",
       " 'en',\n",
       " 'cada',\n",
       " 'uno',\n",
       " 'de',\n",
       " 'los',\n",
       " 'cuerpos',\n",
       " 'cavernosos',\n",
       " 'para',\n",
       " 'lavarlos.',\n",
       " 'Tras',\n",
       " 'la',\n",
       " 'repetición',\n",
       " 'en',\n",
       " 'varias',\n",
       " 'ocasiones',\n",
       " 'de',\n",
       " 'esta',\n",
       " 'última',\n",
       " 'maniobra,',\n",
       " 'se',\n",
       " 'consigue',\n",
       " 'obtener',\n",
       " 'finalmente',\n",
       " 'flaccidez',\n",
       " 'peneana.',\n",
       " 'Durante',\n",
       " 'su',\n",
       " 'permanencia',\n",
       " 'en',\n",
       " 'urgencias',\n",
       " 'y',\n",
       " 'tras',\n",
       " 'lograr',\n",
       " 'flaccidez,',\n",
       " 'se',\n",
       " 'extraen',\n",
       " 'analíticas',\n",
       " 'sanguíneas',\n",
       " 'que',\n",
       " 'revelan',\n",
       " 'una',\n",
       " 'marcada',\n",
       " 'elevación',\n",
       " 'de',\n",
       " 'las',\n",
       " 'cifras',\n",
       " 'de',\n",
       " 'leucocitos',\n",
       " '(414',\n",
       " 'x',\n",
       " '109/L)',\n",
       " 'y',\n",
       " 'plaquetas',\n",
       " '(1100',\n",
       " 'x',\n",
       " '109/L)',\n",
       " 'en',\n",
       " 'sangre.',\n",
       " 'Ante',\n",
       " 'la',\n",
       " 'sospecha',\n",
       " 'de',\n",
       " 'un',\n",
       " 'síndrome',\n",
       " 'mieloproliferativo',\n",
       " 'se',\n",
       " 'consulta',\n",
       " 'con',\n",
       " 'el',\n",
       " 'Servicio',\n",
       " 'de',\n",
       " 'Hematología,',\n",
       " 'que',\n",
       " 'diagnostica',\n",
       " 'leucemia',\n",
       " 'mieloide',\n",
       " 'crónica.',\n",
       " 'Posteriormente',\n",
       " 'el',\n",
       " 'paciente',\n",
       " 'es',\n",
       " 'sometido',\n",
       " 'a',\n",
       " 'un',\n",
       " 'trasplante',\n",
       " 'de',\n",
       " 'médula',\n",
       " 'ósea',\n",
       " 'alogénico',\n",
       " 'de',\n",
       " 'donante',\n",
       " 'familiar',\n",
       " 'con',\n",
       " 'HLA',\n",
       " 'idéntico.',\n",
       " 'La',\n",
       " 'evolución',\n",
       " 'de',\n",
       " 'su',\n",
       " 'enfermedad',\n",
       " 'hematológica',\n",
       " 'es',\n",
       " 'favorable',\n",
       " 'quedando',\n",
       " 'el',\n",
       " 'cuadro',\n",
       " 'de',\n",
       " 'priapismo',\n",
       " 'totalmente',\n",
       " 'resuelto',\n",
       " 'al',\n",
       " 'tratar',\n",
       " 'la',\n",
       " 'leucemia',\n",
       " 'mieloide',\n",
       " 'crónica',\n",
       " 'y',\n",
       " 'sin',\n",
       " 'alteración',\n",
       " 'en',\n",
       " 'la',\n",
       " 'potencia',\n",
       " 'sexual.']"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok = train_raw.iloc[40]['texto']\n",
    "tok.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def acron_detect(concept):\n",
    "#     if ((len(concept) >= 2 and len(concept) <= 10) and sum(x.isupper() for x in concept) >= 1 and any(x.isalpha() for x in concept) and sum(x.isdigit() for x in concept) <= 1) and \\\n",
    "#     (sum(c.isspace() for c in concept) <= 1):\n",
    "#         print(concept, \"is an acronym\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens = train_raw.iloc[50]['texto_POS']\n",
    "# for x in tokens:\n",
    "#     acron_detect(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = 'qiwuei&oqw&'\n",
    "# any(re.findall(r'\\&', s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Paciente', 'NOUN'),\n",
       " ('varón', 'NOUN'),\n",
       " ('47', 'NUM'),\n",
       " ('años', 'NOUN'),\n",
       " ('edad', 'NOUN'),\n",
       " (',', 'PUNCT'),\n",
       " ('hábitos', 'NOUN'),\n",
       " ('tóxicos', 'ADJ'),\n",
       " ('antecedentes', 'NOUN'),\n",
       " ('patológicos', 'ADJ'),\n",
       " ('interés', 'NOUN'),\n",
       " (',', 'PUNCT'),\n",
       " ('acude', 'VERB'),\n",
       " ('consulta', 'NOUN'),\n",
       " ('andrología', 'ADJ'),\n",
       " ('presentar', 'VERB'),\n",
       " ('erecciones', 'NOUN'),\n",
       " ('prolongadas', 'ADJ'),\n",
       " ('dolorosas', 'ADJ'),\n",
       " ('aproximadamente', 'ADV'),\n",
       " ('4', 'NUM'),\n",
       " ('años', 'NOUN'),\n",
       " ('evolución', 'NOUN'),\n",
       " ('tras', 'ADP'),\n",
       " ('traumatismo', 'NOUN'),\n",
       " ('perineal', 'ADJ'),\n",
       " ('cerrado', 'ADJ'),\n",
       " ('manillar', 'VERB'),\n",
       " ('bicicleta', 'NOUN'),\n",
       " ('.', 'PUNCT'),\n",
       " ('En', 'ADP'),\n",
       " ('exploración', 'NOUN'),\n",
       " ('física', 'ADJ'),\n",
       " ('observan', 'VERB'),\n",
       " ('cuerpos', 'NOUN'),\n",
       " ('cavernosos', 'ADJ'),\n",
       " ('aumentados', 'VERB'),\n",
       " ('consistencia', 'NOUN'),\n",
       " (',', 'PUNCT'),\n",
       " ('dolorosos', 'ADJ'),\n",
       " ('palpación', 'NOUN'),\n",
       " (',', 'PUNCT'),\n",
       " ('palpar', 'VERB'),\n",
       " ('pulsos', 'NOUN'),\n",
       " ('anómalos', 'ADJ'),\n",
       " ('.', 'PUNCT'),\n",
       " ('Sensibilidad', 'NOUN'),\n",
       " ('peneana', 'ADJ'),\n",
       " ('conservada', 'ADJ'),\n",
       " ('.', 'PUNCT'),\n",
       " ('Testes', 'NOUN'),\n",
       " ('móviles', 'ADJ'),\n",
       " ('ambas', 'NUM'),\n",
       " ('bolsas', 'NOUN'),\n",
       " ('escrotales', 'ADJ'),\n",
       " ('alteraciones', 'NOUN'),\n",
       " ('.', 'PUNCT'),\n",
       " ('Como', 'SCONJ'),\n",
       " ('exploraciones', 'NOUN'),\n",
       " ('complementarias', 'ADJ'),\n",
       " ('realiza', 'VERB'),\n",
       " ('ecodoppler', 'NOUN'),\n",
       " ('penenano', 'ADJ'),\n",
       " (':', 'PUNCT'),\n",
       " ('vascularización', 'NOUN'),\n",
       " ('cavernosa', 'ADJ'),\n",
       " ('derecha', 'ADJ'),\n",
       " ('aparentemente', 'ADV'),\n",
       " ('conservada', 'ADJ'),\n",
       " (';', 'PUNCT'),\n",
       " ('porción', 'NOUN'),\n",
       " ('proximal', 'ADJ'),\n",
       " ('cuerpo', 'NOUN'),\n",
       " ('cavernoso', 'ADJ'),\n",
       " ('izquierdo', 'ADJ'),\n",
       " ('observa', 'VERB'),\n",
       " ('formación', 'NOUN'),\n",
       " ('anecoica', 'ADJ'),\n",
       " ('(', 'PUNCT'),\n",
       " ('2x1.8x1.5cm', 'NOUN'),\n",
       " (')', 'PUNCT'),\n",
       " ('flujo', 'NOUN'),\n",
       " ('turbulento', 'ADJ'),\n",
       " ('interior', 'ADJ'),\n",
       " ('compatible', 'ADJ'),\n",
       " ('fístula', 'NOUN'),\n",
       " ('arteriovenosa', 'ADJ'),\n",
       " ('(', 'PUNCT'),\n",
       " ('FAV', 'PROPN'),\n",
       " (')', 'PUNCT'),\n",
       " ('larga', 'ADJ'),\n",
       " ('duración', 'NOUN'),\n",
       " ('.', 'PUNCT'),\n",
       " ('Con', 'ADP'),\n",
       " ('orientación', 'NOUN'),\n",
       " ('diagnóstica', 'ADJ'),\n",
       " ('Priapismo', 'PROPN'),\n",
       " ('alto', 'ADJ'),\n",
       " ('flujo', 'NOUN'),\n",
       " ('decide', 'VERB'),\n",
       " ('realización', 'NOUN'),\n",
       " ('Arteriografía', 'PROPN'),\n",
       " ('pudenda', 'ADJ'),\n",
       " ('anestesia', 'ADJ'),\n",
       " ('local', 'ADJ'),\n",
       " ('confirmándose', 'VERB'),\n",
       " ('FAV', 'PROPN'),\n",
       " ('posterior', 'ADJ'),\n",
       " ('embolización', 'NOUN'),\n",
       " ('misma', 'DET'),\n",
       " ('mediante', 'ADP'),\n",
       " ('2', 'NUM'),\n",
       " ('coil', 'NOUN'),\n",
       " ('3x5..', 'NUM'),\n",
       " ('Tras', 'ADP'),\n",
       " ('embolización', 'NOUN'),\n",
       " ('paciente', 'ADJ'),\n",
       " ('evoluciona', 'VERB'),\n",
       " ('favorablemente', 'ADV'),\n",
       " ('detumecencia', 'NOUN'),\n",
       " ('peneana', 'ADJ'),\n",
       " ('completa', 'ADJ'),\n",
       " ('erecciones', 'NOUN'),\n",
       " ('normales', 'ADJ'),\n",
       " ('.', 'PUNCT'),\n",
       " ('Actualmente', 'ADV'),\n",
       " ('asintomático', 'ADJ'),\n",
       " ('.', 'PUNCT')]"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_raw.iloc[40]['doc_id']\n",
    "train_raw.iloc[10]['texto_POS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>StartOffset</th>\n",
       "      <th>EndOffset</th>\n",
       "      <th>Abbreviation</th>\n",
       "      <th>Definition</th>\n",
       "      <th>Definition_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S0210-48062004000500008-1</td>\n",
       "      <td>1650</td>\n",
       "      <td>1652</td>\n",
       "      <td>ml</td>\n",
       "      <td>mililitro</td>\n",
       "      <td>mililitro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S0210-48062004000500008-1</td>\n",
       "      <td>708</td>\n",
       "      <td>709</td>\n",
       "      <td>l</td>\n",
       "      <td>litro</td>\n",
       "      <td>litro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S0210-48062004000500008-1</td>\n",
       "      <td>704</td>\n",
       "      <td>707</td>\n",
       "      <td>mEq</td>\n",
       "      <td>miliequivalente</td>\n",
       "      <td>miliequivalente</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>S0210-48062004000500008-1</td>\n",
       "      <td>677</td>\n",
       "      <td>681</td>\n",
       "      <td>pCO2</td>\n",
       "      <td>presión parcial de co2</td>\n",
       "      <td>presión parcial de co2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>S0210-48062004000500008-1</td>\n",
       "      <td>2287</td>\n",
       "      <td>2290</td>\n",
       "      <td>HLA</td>\n",
       "      <td>human leucocyte antigen</td>\n",
       "      <td>human leucocyte antiger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>S0210-48062004000500008-1</td>\n",
       "      <td>688</td>\n",
       "      <td>692</td>\n",
       "      <td>mmHg</td>\n",
       "      <td>milímetro de mercurio</td>\n",
       "      <td>milímetro de mercurio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>S0210-48062004000500008-1</td>\n",
       "      <td>2031</td>\n",
       "      <td>2032</td>\n",
       "      <td>L</td>\n",
       "      <td>leucocito</td>\n",
       "      <td>leucocito</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>S0210-48062004000500008-1</td>\n",
       "      <td>2004</td>\n",
       "      <td>2005</td>\n",
       "      <td>L</td>\n",
       "      <td>leucocito</td>\n",
       "      <td>leucocito</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>S0210-48062004000500008-1</td>\n",
       "      <td>1581</td>\n",
       "      <td>1583</td>\n",
       "      <td>mg</td>\n",
       "      <td>miligramo</td>\n",
       "      <td>miligramo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      doc_id  StartOffset  EndOffset Abbreviation  \\\n",
       "0  S0210-48062004000500008-1         1650       1652           ml   \n",
       "1  S0210-48062004000500008-1          708        709            l   \n",
       "2  S0210-48062004000500008-1          704        707          mEq   \n",
       "3  S0210-48062004000500008-1          677        681         pCO2   \n",
       "4  S0210-48062004000500008-1         2287       2290          HLA   \n",
       "5  S0210-48062004000500008-1          688        692         mmHg   \n",
       "6  S0210-48062004000500008-1         2031       2032            L   \n",
       "7  S0210-48062004000500008-1         2004       2005            L   \n",
       "8  S0210-48062004000500008-1         1581       1583           mg   \n",
       "\n",
       "                Definition    Definition_lemmatized  \n",
       "0                mililitro                mililitro  \n",
       "1                    litro                    litro  \n",
       "2          miliequivalente          miliequivalente  \n",
       "3   presión parcial de co2   presión parcial de co2  \n",
       "4  human leucocyte antigen  human leucocyte antiger  \n",
       "5    milímetro de mercurio    milímetro de mercurio  \n",
       "6                leucocito                leucocito  \n",
       "7                leucocito                leucocito  \n",
       "8                miligramo                miligramo  "
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_abbr[train_abbr['doc_id'] == 'S0210-48062004000500008-1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#patron = r'\\(([A-Z]{2,8})\\)'\n",
    "#patron1 = r'\\s[A-Z]{1,3}\\s'\n",
    "#patron2 = r'\\s[a-z]{1,3}\\s'\n",
    "patron3 = r'[A-Z]{2,8}'\n",
    "patron4 = r'\\s[a-z]{1,2}\\s'\n",
    "patron5 = r'[a-z]+\\-[A-Z]{1,8}'\n",
    "patron6 = r'[a-z]+\\/[a-z]+'\n",
    "patron7 = r'[A-Z]?[a-z]{1,4}[A-Z]+[a-z]*[1-9]*'\n",
    "patron8 = r'\\/[a-z]*[A-Z]*'\n",
    "\n",
    "\n",
    "# create a list with them\n",
    "regexes = [ patron3, patron4, patron5, patron6, patron7,patron8]\n",
    "for i in regexes:\n",
    "    generic_re = re.compile(\"%s|%s|%s|%s|%s|%s\" % (patron3, patron4, patron5, patron6, patron7,patron8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get Short Formns with a regex in each text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw['abrev'] = train_raw['texto'].map(lambda x: generic_re.findall(x))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
